# ğŸ“˜ Coleta de Dados com Scrapy â€” Mercado Livre Notebooks

Este guia documenta o passo a passo completo de um projeto real de scraping de produtos (notebooks) no site do Mercado Livre, utilizando **Scrapy**, uma das ferramentas mais poderosas de coleta de dados com Python.

---

## ğŸ”§ Etapa 1 â€” Instalar o Scrapy

```bash
pip install scrapy
```

Instalamos o Scrapy, que Ã© um framework robusto para criaÃ§Ã£o de spiders (robÃ´s de coleta de dados). Ele permite navegar por pÃ¡ginas HTML, extrair dados com seletores CSS ou XPath, lidar com paginaÃ§Ã£o, entre outros.

---

## ğŸ—ï¸ Etapa 2 â€” Criar o Projeto

```bash
scrapy startproject mercadolivre
```

Esse comando cria a estrutura inicial do projeto, com os diretÃ³rios:
- `spiders/`: onde ficam os crawlers
- `settings.py`: onde configuramos o comportamento do bot
- `items.py`: (opcional) definiÃ§Ã£o da estrutura de dados
- `middlewares.py`: interceptadores de requisiÃ§Ã£o/resposta

---

## ğŸ“‚ Etapa 3 â€” Entrar no Projeto

```bash
cd mercadolivre
```

---

## ğŸ§¬ Etapa 4 â€” Criar a Spider

```bash
scrapy genspider notebook mercadolivre.com.br
```

Criamos uma spider chamada `notebook` voltada para o domÃ­nio do Mercado Livre. A spider serÃ¡ criada dentro de `mercadolivre/spiders/notebook.py`.

---

## ğŸš Etapa 5 â€” Testar no Shell do Scrapy

```bash
scrapy shell
```

Esse modo nos dÃ¡ um ambiente interativo para testar seletores, verificar HTML da pÃ¡gina e debugar.

---

## ğŸ” Etapa 6 â€” Tentar fazer um `fetch()`

```python
fetch("https://lista.mercadolivre.com.br/notebook?sb=rb#D[A:notebook]")
```

Esse comando busca a pÃ¡gina desejada. Se o retorno for `403 Forbidden`, significa que o site estÃ¡ bloqueando o acesso automatizado.

---

## ğŸ•µï¸ Etapa 7 â€” Adicionar o User-Agent

### O que Ã© `User-Agent`?

Ã‰ uma string que identifica o tipo de cliente que estÃ¡ acessando o site (navegador, sistema, etc). Sites como o Mercado Livre bloqueiam robÃ´s, mas permitem navegadores comuns.

```python
# settings.py
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36"
```

Com isso, nosso bot finge ser um navegador legÃ­timo.

---

## ğŸ§  Etapa 8 â€” Revisar Middlewares (opcional)

```python
# settings.py (mantido padrÃ£o nesse projeto)
DOWNLOADER_MIDDLEWARES = {
   # pode ser customizado caso precise alterar headers, usar proxy, etc.
}
```

---

## ğŸ§ª Etapa 9 â€” Retestando com `scrapy shell`

```python
fetch("https://lista.mercadolivre.com.br/notebook?sb=rb#D[A:notebook]")
response
response.text
```

Verificamos se conseguimos acessar o HTML corretamente e comeÃ§amos a testar os seletores.

---

## ğŸ” Etapa 10 â€” Testar seletores

```python
response.css('div.ui-search-result__wrapper')
```

```python
len(response.css('div.ui-search-result__wrapper'))
```

```python
products = response.css('div.ui-search-result__wrapper')
products[0].css('span.poly-component__brand::text').get()
```

Encontramos os dados e validamos que conseguimos extrair marca, nome, preÃ§o, avaliaÃ§Ãµes, etc.

---

## ğŸ§± Etapa 11 â€” Transformar os testes em cÃ³digo

Criamos o crawler programÃ¡tico com os seletores testados, salvando os dados como JSON.

```bash
scrapy crawl notebook -o data.json
```

---

## ğŸ’¸ Etapa 12 â€” Coletar os preÃ§os corretamente

```python
prices = products[0].css('span.andes-money-amount__fraction::text').getall()
```

Isso retorna todos os valores de preÃ§os (antigo, novo, desconto, etc). Precisamos selecionar corretamente:

```python
old_price = prices[0] if len(prices) > 0 else None
new_price = prices[1] if len(prices) > 1 else None
```

---

## ğŸ“ Etapa 13 â€” Salvar os dados fora da pasta do projeto

```bash
scrapy crawl notebook -o ../data/data.json
```

Organizamos os dados coletados na pasta `data/`, fora do diretÃ³rio `mercadolivre`.

---

## ğŸ“¦ Etapa 14 â€” Organizar o projeto em camadas

Criamos subpastas para manter a estrutura do pipeline mais clara e modular:

```
src/
â”‚
â”œâ”€â”€ coleta/          <- spiders, dados brutos
â”œâ”€â”€ transformacao/   <- tratamento dos dados
â”œâ”€â”€ dashboard/       <- visualizaÃ§Ãµes e relatÃ³rios
```

---

## ğŸ” Etapa 15 â€” PaginaÃ§Ã£o

Adicionamos suporte para mÃºltiplas pÃ¡ginas, buscando o botÃ£o â€œPrÃ³xima PÃ¡ginaâ€:

```python
next_page = response.css('li.andes-pagination__button.andes-pagination__button--next a::attr(href)').get()
yield scrapy.Request(url=next_page, callback=self.parse)
```

- `url` = link para a prÃ³xima pÃ¡gina
- `callback` = funÃ§Ã£o que serÃ¡ chamada ao carregar a nova pÃ¡gina

---

## ğŸ” Etapa 16 â€” Evitar bloqueio

Limitamos a quantidade de pÃ¡ginas para nÃ£o abusar do site:

```python
page_count = 1
max_pages = 10
```

---

## ğŸ¤– Etapa 17 â€” Respeitar `robots.txt`?

Por padrÃ£o:

```python
ROBOTSTXT_OBEY = True
```

Isso impede que o scraper acesse URLs proibidas no `robots.txt` do site. No Mercado Livre, vÃ¡rias URLs sÃ£o bloqueadas por padrÃ£o. Podemos **desabilitar** isso em projetos internos:

```python
ROBOTSTXT_OBEY = False
```

---

## âœ… Etapa 18 â€” Rodar a coleta final

```bash
scrapy crawl notebook -o ../data/data.json
```

Agora, a spider coleta notebooks com nome, marca, preÃ§o, avaliaÃ§Ãµes e vendedor, navegando por atÃ© 10 pÃ¡ginas.

---

## ğŸ Finalizamos a coleta!

O prÃ³ximo passo Ã© criar transformaÃ§Ãµes (por exemplo, limpeza de preÃ§os e normalizaÃ§Ã£o dos nomes) e visualizaÃ§Ãµes (como dashboards com Streamlit, Dash ou Power BI).

Se quiser, posso te ajudar a estruturar a parte de **transformaÃ§Ã£o** ou **visualizaÃ§Ã£o** na pasta `src`.

Quer seguir nessa linha agora?